{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Necessary Packages"
      ],
      "metadata": {
        "id": "Ok1tfoc-mqXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0: Install Necessary Packages\n",
        "!pip install -q gradio                  # Install Gradio for creating web UI\n",
        "!pip install -q yt-dlp                  # Install yt-dlp for downloading YouTube videos\n",
        "!pip install -q openai whisper          # Install OpenAI API client and Whisper for speech recognition\n",
        "!pip install -q youtube-transcript-api  # Install package to fetch YouTube captions (as backup)\n",
        "!pip install -q langchain langchain-community langchain-openai  # Install LangChain ecosystem libraries\n",
        "!pip install -q sentence-transformers   # Install sentence transformers for embedding generation\n",
        "!pip install -q chromadb                # Install ChromaDB for vector storage\n",
        "!pip install -q pydub                   # Install pydub for audio file processing\n",
        "!pip install -q langsmith               # Install LangSmith for testing, evaluation, deployment\n",
        "!pip install -q git+https://github.com/openai/whisper.git  # Install latest Whisper version from GitHub\n",
        "!pip install -q tqdm                    # Install tqdm for progress bars\n"
      ],
      "metadata": {
        "id": "qaRQVRkLb3wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Environment and Import Libraries"
      ],
      "metadata": {
        "id": "9dOkIv0smsl1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2qsvTXlau-0"
      },
      "outputs": [],
      "source": [
        "# Step 1: Setup Environment and Import Libraries\n",
        "import os  # Provides OS-level functionalities\n",
        "import time  # For tracking execution time\n",
        "import textwrap  # For wrapping text nicely\n",
        "import tempfile  # For creating temporary directories/files\n",
        "from pathlib import Path  # Easier file path handling\n",
        "from typing import List, Dict, Any, Optional  # Type hints\n",
        "from tqdm.auto import tqdm  # Progress bar for loops\n",
        "\n",
        "# Audio/Video Processing\n",
        "import yt_dlp  # To download YouTube videos\n",
        "import whisper  # OpenAI Whisper model for transcription\n",
        "from pydub import AudioSegment  # For handling audio files\n",
        "from youtube_transcript_api import YouTubeTranscriptApi  # Backup method for fetching YouTube captions\n",
        "\n",
        "# UI and Interface\n",
        "import gradio as gr  # Gradio for building interactive web apps\n",
        "\n",
        "# Vector Store and Embeddings\n",
        "import chromadb  # Chroma database for storing vectors\n",
        "from sentence_transformers import SentenceTransformer  # Local embedding model for text embeddings\n",
        "\n",
        "# LangChain Components\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter  # To split large text into manageable chunks\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI  # OpenAI interfaces for embeddings and chat models\n",
        "from langchain_community.vectorstores import Chroma  # Chroma wrapper inside LangChain\n",
        "from langchain.chains import RetrievalQA  # Chain for retrieval-based question answering\n",
        "from langchain.prompts import PromptTemplate  # Custom prompt templates\n",
        "from langsmith import Client  # Client for LangSmith testing/tracing\n",
        "\n",
        "# Mount Google Drive if in Colab\n",
        "try:\n",
        "    from google.colab import drive, userdata  # Import Google Colab specific modules\n",
        "    IN_COLAB = True  # Flag indicating Colab environment\n",
        "    drive.mount('/content/drive')  # Mount Google Drive to access secrets or files\n",
        "\n",
        "    # Set API keys from Colab userdata\n",
        "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"  # Enable LangChain tracing\n",
        "    os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")  # Set LangChain API key\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")  # Set OpenAI API key\n",
        "\n",
        "    # Confirm API keys are set\n",
        "    print(\"LANGCHAIN_API_KEY is set:\", os.environ.get(\"LANGCHAIN_API_KEY\") is not None)\n",
        "    print(\"OPENAI_API_KEY is set:\", os.environ.get(\"OPENAI_API_KEY\") is not None)\n",
        "except ImportError:\n",
        "    IN_COLAB = False  # Not running in Colab\n",
        "    # Warn user to manually set API keys\n",
        "    print(\"Not running in Google Colab - make sure API keys are set\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define YouTube Video Downloader"
      ],
      "metadata": {
        "id": "Wd7O3LDHmv6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define YouTube Video Downloader\n",
        "class YouTubeProcessor:\n",
        "    def __init__(self, output_dir: str = \"./downloads\"):\n",
        "        \"\"\"Initialize with an output directory for downloaded files.\"\"\"\n",
        "        self.output_dir = output_dir  # Store output directory path\n",
        "        Path(output_dir).mkdir(exist_ok=True)  # Create output directory if it doesn't exist\n",
        "\n",
        "        # yt-dlp configuration\n",
        "        self.ydl_opts = {\n",
        "            'format': 'bestaudio/best',  # Download best available audio format\n",
        "            'outtmpl': f'{output_dir}/audio_%(id)s.%(ext)s',  # Output filename template\n",
        "            'postprocessors': [{\n",
        "                'key': 'FFmpegExtractAudio',  # Extract audio using FFmpeg\n",
        "                'preferredcodec': 'mp3',  # Convert audio to mp3\n",
        "                'preferredquality': '192',  # Set quality to 192kbps\n",
        "            }],\n",
        "            'quiet': True  # Suppress detailed output\n",
        "        }\n",
        "\n",
        "    def extract_video_id(self, url: str) -> str:\n",
        "        \"\"\"Extract YouTube video ID from URL.\"\"\"\n",
        "        if \"youtu.be\" in url:  # Short URL format\n",
        "            video_id = url.split(\"/\")[-1].split(\"?\")[0]  # Extract ID\n",
        "        elif \"youtube.com\" in url:  # Standard YouTube URL\n",
        "            video_id = url.split(\"v=\")[1].split(\"&\")[0]  # Extract ID\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid YouTube URL: {url}\")  # Raise error if URL format is wrong\n",
        "        return video_id\n",
        "\n",
        "    def download_videos(self, video_urls: List[str]) -> List[str]:\n",
        "        \"\"\"Download audio from YouTube videos and return file paths.\"\"\"\n",
        "        downloaded_files = []  # Initialize empty list for downloaded file paths\n",
        "\n",
        "        print(f\"Downloading {len(video_urls)} videos...\")  # Inform how many videos to download\n",
        "        with yt_dlp.YoutubeDL(self.ydl_opts) as ydl:  # Initialize yt-dlp with options\n",
        "            for url in tqdm(video_urls, desc=\"Downloading\"):  # Show progress bar\n",
        "                video_id = self.extract_video_id(url)  # Get video ID\n",
        "                file_path = f\"{self.output_dir}/audio_{video_id}.mp3\"  # Expected file path\n",
        "\n",
        "                # Skip if already downloaded\n",
        "                if Path(file_path).exists():\n",
        "                    print(f\"File already exists: {file_path}\")  # Inform if already downloaded\n",
        "                    downloaded_files.append(file_path)\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    ydl.download([url])  # Download the audio\n",
        "                    downloaded_files.append(file_path)  # Add to list\n",
        "                except Exception as e:\n",
        "                    print(f\"Error downloading {url}: {e}\")  # Print error if download fails\n",
        "\n",
        "                    # Try to get captions as fallback\n",
        "                    try:\n",
        "                        transcript = YouTubeTranscriptApi.get_transcript(video_id)  # Attempt to fetch transcript\n",
        "                        transcript_text = \" \".join([entry[\"text\"] for entry in transcript])  # Combine transcript lines\n",
        "                        text_file = f\"{self.output_dir}/transcript_{video_id}.txt\"  # Save as text file\n",
        "                        with open(text_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                            f.write(transcript_text)\n",
        "                        print(f\"Retrieved transcript instead: {text_file}\")  # Inform user\n",
        "                    except Exception as e2:\n",
        "                        print(f\"Failed to get transcript too: {e2}\")  # Print second error if both fail\n",
        "\n",
        "        return downloaded_files  # Return list of successfully downloaded files\n"
      ],
      "metadata": {
        "id": "4aFaXSjYbZCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Transcription with Whisper"
      ],
      "metadata": {
        "id": "rL8FefF6myDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Audio Transcription with Whisper\n",
        "class TranscriptionProcessor:\n",
        "    def __init__(self, model_size: str = \"base\"):\n",
        "        \"\"\"Initialize with specified Whisper model size.\"\"\"\n",
        "        print(f\"Loading Whisper model ({model_size})...\")  # Log model loading\n",
        "        self.model = whisper.load_model(model_size)  # Load Whisper model\n",
        "        print(\"Whisper model loaded\")  # Confirm model is ready\n",
        "\n",
        "    def transcribe_audio(self, audio_files: List[str]) -> Dict[str, str]:\n",
        "        \"\"\"Transcribe audio files and return dict of {file_path: transcript}.\"\"\"\n",
        "        transcripts = {}  # Initialize transcripts dictionary\n",
        "\n",
        "        for audio_file in tqdm(audio_files, desc=\"Transcribing\"):  # Loop with progress bar\n",
        "            text_filename = audio_file.replace(\".mp3\", \".txt\")  # Expected transcript file path\n",
        "\n",
        "            # Skip if transcript already exists\n",
        "            if Path(text_filename).exists():\n",
        "                with open(text_filename, \"r\", encoding=\"utf-8\") as f:\n",
        "                    transcripts[audio_file] = f.read()  # Load existing transcript\n",
        "                print(f\"Using existing transcript: {text_filename}\")  # Inform user\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                result = self.model.transcribe(audio_file)  # Transcribe audio\n",
        "                with open(text_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "                    f.write(result['text'])  # Save transcription to file\n",
        "                transcripts[audio_file] = result['text']  # Add to dictionary\n",
        "                print(f\"Transcribed {audio_file} → {text_filename}\")  # Inform success\n",
        "            except Exception as e:\n",
        "                print(f\"Error transcribing {audio_file}: {e}\")  # Handle transcription error\n",
        "\n",
        "        return transcripts  # Return dict of transcripts\n"
      ],
      "metadata": {
        "id": "M7NlnDu7baE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Database Management"
      ],
      "metadata": {
        "id": "kmioQvv5m0UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Vector Database Management\n",
        "class VectorDatabaseManager:\n",
        "    def __init__(\n",
        "        self,\n",
        "        persist_directory: str = \"./chroma_store\",\n",
        "        collection_name: str = \"youtube_transcripts\",\n",
        "        chunk_size: int = 500,\n",
        "        chunk_overlap: int = 50\n",
        "    ):\n",
        "        \"\"\"Initialize the vector database manager.\"\"\"\n",
        "        self.persist_directory = persist_directory  # Where to save Chroma DB\n",
        "        self.collection_name = collection_name  # Collection name in Chroma\n",
        "        self.chunk_size = chunk_size  # Chunk size for text splitting\n",
        "        self.chunk_overlap = chunk_overlap  # Overlap between text chunks\n",
        "\n",
        "        Path(persist_directory).mkdir(exist_ok=True)  # Create persistence directory if needed\n",
        "\n",
        "        self.local_embed_model = SentenceTransformer('all-MiniLM-L6-v2')  # Local embedding model for fallback\n",
        "\n",
        "        try:\n",
        "            self.openai_embed_model = OpenAIEmbeddings()  # Try to load OpenAI embeddings\n",
        "            self.use_openai_embeddings = True  # Use OpenAI if available\n",
        "            print(\"Using OpenAI embeddings\")  # Inform user\n",
        "        except Exception:\n",
        "            self.use_openai_embeddings = False  # Otherwise fallback\n",
        "            print(\"Falling back to local embeddings\")  # Inform fallback\n",
        "\n",
        "        self.client = chromadb.PersistentClient(path=persist_directory)  # Initialize Chroma client\n",
        "        self.collection = self.client.get_or_create_collection(name=collection_name)  # Create/Get collection\n",
        "\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len\n",
        "        )  # Create a text splitter\n",
        "\n",
        "    def store_transcripts(self, transcripts: Dict[str, str]) -> None:\n",
        "        \"\"\"Store transcript texts in vector database.\"\"\"\n",
        "        if not transcripts:\n",
        "            print(\"No transcripts to store\")  # Exit if nothing to store\n",
        "            return\n",
        "\n",
        "        all_chunks = []  # Store all text chunks\n",
        "        all_metadatas = []  # Store corresponding metadata\n",
        "\n",
        "        for file_path, text in transcripts.items():  # Iterate transcripts\n",
        "            source_id = Path(file_path).stem  # Unique ID per file\n",
        "\n",
        "            chunks = self.text_splitter.split_text(text)  # Split transcript into chunks\n",
        "            metadatas = [{\"source\": source_id, \"chunk_id\": i} for i in range(len(chunks))]  # Metadata per chunk\n",
        "\n",
        "            all_chunks.extend(chunks)  # Add to chunk list\n",
        "            all_metadatas.extend(metadatas)  # Add to metadata list\n",
        "\n",
        "        # Generate and store embeddings\n",
        "        if self.use_openai_embeddings:\n",
        "            vectordb = Chroma(  # Use LangChain's Chroma wrapper\n",
        "                persist_directory=self.persist_directory,\n",
        "                embedding_function=self.openai_embed_model,\n",
        "                collection_name=self.collection_name\n",
        "            )\n",
        "            vectordb.add_texts(texts=all_chunks, metadatas=all_metadatas)  # Add texts\n",
        "            vectordb.persist()  # Save changes\n",
        "        else:\n",
        "            embeddings = self.local_embed_model.encode(all_chunks).tolist()  # Local embeddings\n",
        "\n",
        "            ids = [f\"chunk_{i}\" for i in range(len(all_chunks))]  # IDs for each chunk\n",
        "            self.collection.add(\n",
        "                documents=all_chunks,\n",
        "                embeddings=embeddings,\n",
        "                metadatas=all_metadatas,\n",
        "                ids=ids\n",
        "            )\n",
        "\n",
        "        print(f\"Stored {len(all_chunks)} chunks in Chroma vector database\")  # Confirm completion\n",
        "\n",
        "    def get_retriever(self, top_k: int = 5):\n",
        "        \"\"\"Get a retriever for the vector database.\"\"\"\n",
        "        if self.use_openai_embeddings:\n",
        "            vectordb = Chroma(\n",
        "                persist_directory=self.persist_directory,\n",
        "                embedding_function=self.openai_embed_model,\n",
        "                collection_name=self.collection_name\n",
        "            )\n",
        "            return vectordb.as_retriever(search_kwargs={\"k\": top_k})  # Return retriever\n",
        "        else:\n",
        "            return None  # No retriever if local embeddings used (needs extra code)\n"
      ],
      "metadata": {
        "id": "VAE9M7enbmh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Answering Chain"
      ],
      "metadata": {
        "id": "PEJOKXSwm4FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Question Answering Chain\n",
        "class MedicalAssistantQA:\n",
        "    def __init__(\n",
        "        self,\n",
        "        vector_db_manager: VectorDatabaseManager,\n",
        "        model_name: str = \"gpt-3.5-turbo\",\n",
        "        temperature: float = 0\n",
        "    ):\n",
        "        \"\"\"Initialize the QA system.\"\"\"\n",
        "        self.vector_db_manager = vector_db_manager  # Store vector DB manager\n",
        "\n",
        "        try:\n",
        "            self.llm = ChatOpenAI(temperature=temperature, model_name=model_name)  # Initialize ChatOpenAI model\n",
        "            self.has_llm = True  # Model is available\n",
        "            print(f\"Using {model_name} for answering\")  # Inform model use\n",
        "        except Exception as e:\n",
        "            self.has_llm = False  # Flag unavailable\n",
        "            print(f\"Error initializing ChatOpenAI: {e}\")  # Print error\n",
        "            print(\"QA functionality will be limited\")  # Warn\n",
        "\n",
        "        self.retriever = vector_db_manager.get_retriever(top_k=5)  # Initialize retriever\n",
        "\n",
        "        if self.has_llm and self.retriever:\n",
        "            self.prompt_template = PromptTemplate(\n",
        "                input_variables=[\"context\", \"question\"],\n",
        "                template=\"\"\"\n",
        "                Use the following information to answer the question accurately and scientifically.\n",
        "                If you don't know, say \"I don't know based on the available information.\"\n",
        "\n",
        "                ----\n",
        "                Information:\n",
        "                {context}\n",
        "                ----\n",
        "                Question: {question}\n",
        "                Answer:\n",
        "                \"\"\"\n",
        "            )  # Define retrieval-augmented prompt template\n",
        "\n",
        "            self.qa_chain = RetrievalQA.from_chain_type(\n",
        "                llm=self.llm,\n",
        "                chain_type=\"stuff\",\n",
        "                retriever=self.retriever,\n",
        "                chain_type_kwargs={\"prompt\": self.prompt_template}\n",
        "            )  # Create retrieval QA chain\n",
        "            print(\"QA Chain initialized successfully\")  # Inform success\n",
        "        else:\n",
        "            self.qa_chain = None  # Fallback if something fails\n",
        "            print(\"QA Chain could not be initialized\")  # Inform\n",
        "\n",
        "    def answer_question(self, question: str) -> str:\n",
        "        \"\"\"Answer a question using the QA chain.\"\"\"\n",
        "        if not self.qa_chain:\n",
        "            return \"Sorry, I'm not able to answer questions right now due to missing components.\"\n",
        "\n",
        "        try:\n",
        "            start_time = time.time()  # Start timer\n",
        "            response = self.qa_chain.run(question)  # Run QA chain\n",
        "            end_time = time.time()  # End timer\n",
        "            latency = end_time - start_time  # Calculate latency\n",
        "            print(f\"Question answered in {latency:.2f} seconds\")  # Print latency\n",
        "            return response  # Return model answer\n",
        "        except Exception as e:\n",
        "            print(f\"Error answering question: {e}\")  # Print error\n",
        "            return f\"Sorry, I encountered an error: {str(e)}\"  # Return error string\n"
      ],
      "metadata": {
        "id": "rv11PFe-bq1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speech-to-Text Processing for User Input"
      ],
      "metadata": {
        "id": "bSugG4B6m51z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Speech-to-Text Processing for User Input\n",
        "class SpeechInputProcessor:\n",
        "    def __init__(self, whisper_model_size: str = \"base\"):\n",
        "        \"\"\"Initialize speech input processor with Whisper.\"\"\"\n",
        "        print(f\"Loading Whisper model for speech input ({whisper_model_size})...\")  # Inform loading\n",
        "        self.model = whisper.load_model(whisper_model_size)  # Load Whisper model\n",
        "        print(\"Speech input model loaded\")  # Confirm loading\n",
        "\n",
        "        self.temp_dir = tempfile.mkdtemp()  # Create a temporary directory for any temporary audio files\n",
        "        print(f\"Temporary directory for speech files: {self.temp_dir}\")  # Inform temp directory\n",
        "\n",
        "    def transcribe_speech(self, audio_data) -> str:\n",
        "        \"\"\"Transcribe speech from audio data to text.\"\"\"\n",
        "        if audio_data is None:\n",
        "            return \"\"  # If no audio is given, return empty\n",
        "\n",
        "        try:\n",
        "            # In Gradio, audio_data is usually a filepath\n",
        "            if isinstance(audio_data, str):\n",
        "                audio_path = audio_data  # Use the path directly\n",
        "            else:\n",
        "                if isinstance(audio_data, tuple) and len(audio_data) >= 1:\n",
        "                    audio_path = audio_data[0]  # Use first item if tuple\n",
        "                else:\n",
        "                    temp_file = Path(self.temp_dir) / \"temp_input.wav\"  # Path to temporary file\n",
        "                    if hasattr(audio_data, 'save'):\n",
        "                        audio_data.save(temp_file)  # Save audio object if possible\n",
        "                        audio_path = str(temp_file)\n",
        "                    else:\n",
        "                        print(\"Unsupported audio data format\")  # Log unsupported format\n",
        "                        return \"\"\n",
        "\n",
        "            print(f\"Transcribing audio from: {audio_path}\")  # Debug message\n",
        "            result = self.model.transcribe(audio_path)  # Perform transcription\n",
        "            transcribed_text = result['text'].strip()  # Extract and clean text\n",
        "            print(f\"Transcribed speech: {transcribed_text}\")  # Log result\n",
        "            return transcribed_text  # Return transcription\n",
        "        except Exception as e:\n",
        "            print(f\"Error transcribing speech: {e}\")  # Log error\n",
        "            import traceback\n",
        "            traceback.print_exc()  # Show full stack trace\n",
        "            return \"\"  # Return empty string on failure\n"
      ],
      "metadata": {
        "id": "IrbMNWg4buAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Application Class"
      ],
      "metadata": {
        "id": "0bb7nUI2m92B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Main Application Class\n",
        "class MedicalAssistantApp:\n",
        "    def __init__(self, video_urls: List[str]):\n",
        "        \"\"\"Initialize the complete application.\"\"\"\n",
        "        self.video_urls = video_urls  # Store list of video URLs\n",
        "\n",
        "        # Initialize components\n",
        "        self.youtube_processor = YouTubeProcessor()  # Handles video download\n",
        "        self.transcription_processor = TranscriptionProcessor(model_size=\"base\")  # Handles audio transcription\n",
        "        self.vector_db_manager = VectorDatabaseManager()  # Manages vector DB\n",
        "        self.qa_system = MedicalAssistantQA(self.vector_db_manager)  # QA model setup\n",
        "        self.speech_processor = SpeechInputProcessor(whisper_model_size=\"base\")  # Speech input processor\n",
        "\n",
        "        self.initialize_knowledge_base()  # Prepare the vector database with YouTube content\n",
        "\n",
        "    def initialize_knowledge_base(self):\n",
        "        \"\"\"Download videos, transcribe, and store in vector database.\"\"\"\n",
        "        audio_files = self.youtube_processor.download_videos(self.video_urls)  # Download audio files\n",
        "\n",
        "        # Check if there are already text transcripts available\n",
        "        transcript_files = list(Path(\"./downloads\").glob(\"transcript_*.txt\"))  # List any existing transcripts\n",
        "        transcripts = {}  # Dict to store transcripts\n",
        "\n",
        "        for file in transcript_files:  # Read all existing transcript files\n",
        "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "                transcripts[str(file)] = f.read()\n",
        "\n",
        "        audio_transcripts = self.transcription_processor.transcribe_audio(audio_files)  # Transcribe audios\n",
        "        transcripts.update(audio_transcripts)  # Merge audio transcripts\n",
        "\n",
        "        self.vector_db_manager.store_transcripts(transcripts)  # Store all transcripts into vector database\n",
        "\n",
        "    def answer_question(self, question: str) -> str:\n",
        "        \"\"\"Answer a question using the QA system.\"\"\"\n",
        "        return self.qa_system.answer_question(question)  # Direct call to QA module\n",
        "\n",
        "    def process_speech_input(self, audio_data) -> str:\n",
        "        \"\"\"Process speech input to text.\"\"\"\n",
        "        return self.speech_processor.transcribe_speech(audio_data)  # Transcribe audio input\n",
        "\n",
        "    def launch_ui(self):\n",
        "        \"\"\"Launch the Gradio interface.\"\"\"\n",
        "        # Define callback functions for Gradio\n",
        "        def text_input(message, chat_history):\n",
        "            response = self.answer_question(message)  # Get answer for text input\n",
        "            chat_history.append((message, response))  # Update chat\n",
        "            return \"\", chat_history  # Clear textbox after submission\n",
        "\n",
        "        def speech_input(audio, chat_history):\n",
        "            if audio is None:\n",
        "                return chat_history  # If no audio, return unchanged\n",
        "\n",
        "            print(f\"Received audio input: {type(audio)}\")  # Debug\n",
        "\n",
        "            transcribed_text = self.process_speech_input(audio)  # Transcribe speech\n",
        "\n",
        "            if not transcribed_text:\n",
        "                chat_history.append((\"\", \"I couldn't understand the audio. Please try again.\"))\n",
        "                return chat_history  # Handle empty transcription\n",
        "\n",
        "            response = self.answer_question(transcribed_text)  # Get model answer\n",
        "            chat_history.append((transcribed_text, response))  # Update chat\n",
        "            return chat_history\n",
        "\n",
        "        # Create Gradio Blocks app\n",
        "        with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\")) as demo:\n",
        "            gr.Markdown(\n",
        "                \"\"\"\n",
        "                # Smart Medical Assistant\n",
        "                **Ask any question related to medical educational videos!**\n",
        "                Topics: Blood Pressure - CPR - Diabetes\n",
        "\n",
        "                You can type your question or click the microphone to speak!\n",
        "                \"\"\",\n",
        "                elem_id=\"title\"\n",
        "            )  # App title and instructions\n",
        "\n",
        "            chatbot = gr.Chatbot(height=450, elem_id=\"chatbot\")  # Chatbot area\n",
        "\n",
        "            with gr.Tab(\"Text Input\"):  # Text input tab\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=8):\n",
        "                        msg = gr.Textbox(\n",
        "                            show_label=False,\n",
        "                            placeholder=\"Type your question here...\",\n",
        "                            elem_id=\"input-text\"\n",
        "                        )\n",
        "                    with gr.Column(scale=2):\n",
        "                        submit_btn = gr.Button(\"Send\", elem_id=\"send-btn\")  # Submit button\n",
        "\n",
        "            with gr.Tab(\"Voice Input\"):  # Voice input tab\n",
        "                audio_input = gr.Audio(\n",
        "                    sources=[\"microphone\"],\n",
        "                    type=\"filepath\",  # Important: store as filepath\n",
        "                    label=\"Speak your question\"\n",
        "                )\n",
        "                submit_audio_btn = gr.Button(\"Process Voice Input\")  # Process speech button\n",
        "\n",
        "            clear_btn = gr.Button(\"Clear Chat\")  # Button to reset chat\n",
        "\n",
        "            # Event handlers for buttons\n",
        "            submit_btn.click(text_input, inputs=[msg, chatbot], outputs=[msg, chatbot])  # Text submission\n",
        "            msg.submit(text_input, inputs=[msg, chatbot], outputs=[msg, chatbot])  # Press Enter in textbox\n",
        "\n",
        "            # Function to process speech input\n",
        "            def process_voice_and_respond(audio_file, history):\n",
        "                print(f\"Processing voice input: {audio_file}\")  # Debug\n",
        "\n",
        "                if audio_file is None:\n",
        "                    history.append((\"\", \"No audio detected. Please try again.\"))\n",
        "                    return history\n",
        "\n",
        "                transcribed_text = self.process_speech_input(audio_file)  # Transcribe\n",
        "\n",
        "                if not transcribed_text:\n",
        "                    history.append((\"\", \"I couldn't understand the audio. Please try again.\"))\n",
        "                    return history\n",
        "\n",
        "                response = self.answer_question(transcribed_text)  # Answer\n",
        "                history.append((f\"🎤: {transcribed_text}\", response))  # Update chat\n",
        "                return history\n",
        "\n",
        "            submit_audio_btn.click(  # Connect audio button\n",
        "                process_voice_and_respond,\n",
        "                inputs=[audio_input, chatbot],\n",
        "                outputs=[chatbot]\n",
        "            )\n",
        "            clear_btn.click(lambda: (\"\", []), outputs=[msg, chatbot])  # Clear chat button\n",
        "\n",
        "            gr.Examples(  # Examples for quick tests\n",
        "                examples=[\n",
        "                    \"What are the symptoms of diabetes?\",\n",
        "                    \"How do I check blood pressure correctly?\",\n",
        "                    \"What are the steps for CPR?\",\n",
        "                    \"What are normal blood pressure ranges?\",\n",
        "                    \"How is type 1 diabetes different from type 2?\"\n",
        "                ],\n",
        "                inputs=msg\n",
        "            )\n",
        "\n",
        "        demo.launch(share=True)  # Launch Gradio app, generate shareable public link\n"
      ],
      "metadata": {
        "id": "vW2WDcqZbwl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the Application"
      ],
      "metadata": {
        "id": "DynO9z2DnAaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Running the Application\n",
        "if __name__ == \"__main__\":\n",
        "    # Define YouTube URLs for medical education videos\n",
        "    video_urls = [\n",
        "        \"https://www.youtube.com/watch?v=DUaxt8OlT3o\",  # Blood Pressure video\n",
        "        \"https://www.youtube.com/watch?v=Kg0bn23S_vw\",  # CPR video\n",
        "        \"https://www.youtube.com/watch?v=wZAjVQWbMlE\"   # Diabetes video\n",
        "    ]\n",
        "\n",
        "    # Create and launch the application\n",
        "    app = MedicalAssistantApp(video_urls)  # Initialize app with videos\n",
        "    app.launch_ui()  # Launch the Gradio user interface\n"
      ],
      "metadata": {
        "id": "rt8vq9xZbz88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhanced LangSmith Evaluation"
      ],
      "metadata": {
        "id": "LGfYELpSnCaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Enhanced LangSmith Evaluation\n",
        "def evaluate_with_langsmith(\n",
        "    qa_system,\n",
        "    test_questions: List[str],\n",
        "    dataset_name: str = \"Medical QA Eval\",\n",
        "    evaluate_metrics: bool = True,\n",
        "    save_results: bool = True,\n",
        "    output_file: str = \"evaluation_results.json\"\n",
        "):\n",
        "    from uuid import uuid4  # For random suffix\n",
        "\n",
        "    print(f\"\\n{'='*8} STARTING EVALUATION {'='*8}\")\n",
        "    print(f\"Evaluating {len(test_questions)} questions...\")\n",
        "\n",
        "    evaluation_results = {\n",
        "        \"summary\": {\n",
        "            \"total_questions\": len(test_questions),\n",
        "            \"successful_responses\": 0,\n",
        "            \"failed_responses\": 0,\n",
        "            \"avg_response_time\": 0,\n",
        "            \"empty_responses\": 0,\n",
        "            \"uncertain_responses\": 0\n",
        "        },\n",
        "        \"detailed_results\": []\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        client = Client()\n",
        "        dataset_name_with_id = f\"{dataset_name}-{str(uuid4())[:8]}\"\n",
        "        dataset = client.create_dataset(\n",
        "            dataset_name=dataset_name_with_id,\n",
        "            description=\"Evaluation of Smart Medical Assistant QA system.\"\n",
        "        )\n",
        "        dataset_id = dataset.id\n",
        "        print(f\"Created LangSmith dataset: {dataset_name_with_id}\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ LangSmith setup failed: {e}\")\n",
        "        client = None\n",
        "        dataset_id = None\n",
        "\n",
        "    total_time = 0\n",
        "\n",
        "    for question in tqdm(test_questions, desc=\"Evaluating\"):\n",
        "        result = {\"question\": question, \"answer\": None, \"response_time\": None, \"error\": None}\n",
        "\n",
        "        try:\n",
        "            start = time.time()\n",
        "            answer = qa_system.answer_question(question)\n",
        "            end = time.time()\n",
        "\n",
        "            result[\"answer\"] = answer\n",
        "            result[\"response_time\"] = end - start\n",
        "            total_time += result[\"response_time\"]\n",
        "            evaluation_results[\"summary\"][\"successful_responses\"] += 1\n",
        "\n",
        "            if not answer.strip():\n",
        "                evaluation_results[\"summary\"][\"empty_responses\"] += 1\n",
        "            if any(p in answer.lower() for p in [\"i don't know\", \"not available\"]):\n",
        "                evaluation_results[\"summary\"][\"uncertain_responses\"] += 1\n",
        "\n",
        "            if client and dataset_id:\n",
        "                client.create_example(\n",
        "                    inputs={\"question\": question},\n",
        "                    outputs={\"answer\": answer},\n",
        "                    dataset_id=dataset_id\n",
        "                )\n",
        "\n",
        "            print(f\"Q: {question}\\nA: {answer}\\nTime: {result['response_time']:.2f}s\\n{'-'*40}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            result[\"error\"] = str(e)\n",
        "            evaluation_results[\"summary\"][\"failed_responses\"] += 1\n",
        "            print(f\"❌ Error with '{question}': {e}\")\n",
        "\n",
        "        evaluation_results[\"detailed_results\"].append(result)\n",
        "\n",
        "    if evaluation_results[\"summary\"][\"successful_responses\"]:\n",
        "        evaluation_results[\"summary\"][\"avg_response_time\"] = total_time / evaluation_results[\"summary\"][\"successful_responses\"]\n",
        "\n",
        "    print(\"\\nEVALUATION SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "    for k, v in evaluation_results[\"summary\"].items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "    if save_results:\n",
        "        try:\n",
        "            import json\n",
        "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(evaluation_results, f, indent=2, ensure_ascii=False)\n",
        "            print(f\"✅ Results saved to {output_file}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Could not save results: {e}\")\n",
        "\n",
        "    return evaluation_results  # <--- THIS returns the results properly!\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q6jEpsTnmPkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    # Step 9.1\n",
        "    evaluation_data = evaluate_with_langsmith(\n",
        "    app.qa_system,\n",
        "    [\n",
        "    # Blood Pressure Focus\n",
        "    \"What is considered a high blood pressure reading?\",\n",
        "    \"Can stress cause high blood pressure?\",\n",
        "    \"What lifestyle changes help lower blood pressure?\",\n",
        "\n",
        "    # CPR Focus\n",
        "    \"What is the first step when performing CPR?\",\n",
        "    \"Is mouth-to-mouth necessary when performing CPR?\",\n",
        "    \"How fast should chest compressions be during CPR?\",\n",
        "\n",
        "    # Diabetes Focus\n",
        "    \"How does type 1 diabetes differ from type 2 diabetes?\",\n",
        "    \"What dietary recommendations are there for diabetics?\",\n",
        "    \"What are early symptoms of diabetes?\",\n",
        "\n",
        "    # General Medical / Critical Thinking\n",
        "    \"Is there a connection between CPR effectiveness and diabetes?\"\n",
        "    ],\n",
        "    dataset_name=\"Medical QA Eval\"\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0yyraVgSPIfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    #Step 9.2\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "success = evaluation_data[\"summary\"][\"successful_responses\"]\n",
        "failed = evaluation_data[\"summary\"][\"failed_responses\"]\n",
        "uncertain = evaluation_data[\"summary\"][\"uncertain_responses\"]\n",
        "empty = evaluation_data[\"summary\"][\"empty_responses\"]\n",
        "\n",
        "labels = [\"Successful\", \"Failed\", \"Uncertain\", \"Empty\"]\n",
        "values = [success, failed, uncertain, empty]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.bar(labels, values, color=[\"green\", \"red\", \"orange\", \"gray\"])\n",
        "plt.title(\"Evaluation Results Summary\")\n",
        "plt.ylabel(\"Number of Responses\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "JZlBzhyHPJjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10\n",
        "# Visualization 1: System Architecture Diagram\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from matplotlib.patches import Rectangle, FancyArrowPatch, Circle\n",
        "\n",
        "def create_system_architecture_diagram():\n",
        "    \"\"\"Create a visual representation of the system architecture\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "    ax.set_xlim(0, 100)\n",
        "    ax.set_ylim(0, 60)\n",
        "\n",
        "    # Turn off axis\n",
        "    ax.set_axis_off()\n",
        "\n",
        "    # Add title\n",
        "    plt.title(\"Smart Medical Assistant - System Architecture\", fontsize=18, pad=20)\n",
        "\n",
        "    # Component colors\n",
        "    colors = {\n",
        "        'youtube': '#FF0000',  # YouTube red\n",
        "        'audio': '#9C27B0',    # Purple\n",
        "        'text': '#2196F3',     # Blue\n",
        "        'vector': '#FF9800',   # Orange\n",
        "        'qa': '#4CAF50',       # Green\n",
        "        'ui': '#607D8B'        # Blue-gray\n",
        "    }\n",
        "\n",
        "    # Draw components\n",
        "    components = [\n",
        "        # (name, x, y, width, height, color)\n",
        "        ('YouTube Videos', 10, 45, 15, 10, colors['youtube']),\n",
        "        ('Audio Processing', 30, 45, 15, 10, colors['audio']),\n",
        "        ('Transcription', 50, 45, 15, 10, colors['text']),\n",
        "        ('Vector Database', 40, 25, 15, 10, colors['vector']),\n",
        "        ('QA System', 70, 25, 15, 10, colors['qa']),\n",
        "        ('User Interface', 50, 5, 30, 10, colors['ui'])\n",
        "    ]\n",
        "\n",
        "    for name, x, y, width, height, color in components:\n",
        "        rect = Rectangle((x, y), width, height, facecolor=color, alpha=0.7, edgecolor='black')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x + width/2, y + height/2, name, ha='center', va='center', fontweight='bold')\n",
        "\n",
        "    # Draw arrows\n",
        "    arrows = [\n",
        "        # (start_x, start_y, end_x, end_y, color)\n",
        "        (25, 50, 30, 50, colors['youtube']),  # YouTube to Audio\n",
        "        (45, 50, 50, 50, colors['audio']),    # Audio to Transcription\n",
        "        (57.5, 45, 47.5, 35, colors['text']),  # Transcription to Vector DB\n",
        "        (55, 30, 70, 30, colors['vector']),   # Vector DB to QA\n",
        "        (77.5, 25, 77.5, 10, colors['qa']),   # QA to UI\n",
        "        (65, 5, 20, 5, 'gray'),               # UI to User (input)\n",
        "        (20, 10, 50, 10, 'gray'),             # User to UI (output)\n",
        "    ]\n",
        "\n",
        "    for start_x, start_y, end_x, end_y, color in arrows:\n",
        "        arrow = FancyArrowPatch((start_x, start_y), (end_x, end_y),\n",
        "                                arrowstyle='-|>', mutation_scale=15,\n",
        "                                color=color, linewidth=2)\n",
        "        ax.add_patch(arrow)\n",
        "\n",
        "    # Add user icon\n",
        "    user_circle = Circle((15, 8), 5, facecolor='#E0E0E0', edgecolor='black')\n",
        "    ax.add_patch(user_circle)\n",
        "    ax.text(15, 8, \"User\", ha='center', va='center', fontweight='bold')\n",
        "\n",
        "    # Add explanatory text\n",
        "    annotations = [\n",
        "        (15, 35, \"Source Videos\", colors['youtube']),\n",
        "        (35, 35, \"Audio Extraction\", colors['audio']),\n",
        "        (55, 35, \"Whisper Model\", colors['text']),\n",
        "        (30, 15, \"Knowledge Base\", colors['vector']),\n",
        "        (85, 20, \"OpenAI GPT\", colors['qa']),\n",
        "        (85, 5, \"Gradio Interface\", colors['ui'])\n",
        "    ]\n",
        "\n",
        "    for x, y, text, color in annotations:\n",
        "        ax.text(x, y, text, ha='center', va='center', color=color,\n",
        "                bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.5'))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('system_architecture.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return \"System architecture diagram created\"\n",
        "\n",
        "# Visualization 2: Workflow Process Flowchart\n",
        "def create_workflow_diagram():\n",
        "    \"\"\"Create a flowchart showing the process workflow\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "    ax.set_xlim(0, 100)\n",
        "    ax.set_ylim(0, 100)\n",
        "\n",
        "    # Turn off axis\n",
        "    ax.set_axis_off()\n",
        "\n",
        "    # Add title\n",
        "    plt.title(\"Medical Assistant Workflow\", fontsize=18, pad=20)\n",
        "\n",
        "    # Define process steps\n",
        "    steps = [\n",
        "        # (name, x, y, width, height)\n",
        "        (\"YouTube Video Selection\", 30, 90, 40, 8),\n",
        "        (\"Video Download\", 30, 80, 40, 8),\n",
        "        (\"Audio Extraction\", 30, 70, 40, 8),\n",
        "        (\"Speech Transcription\", 30, 60, 40, 8),\n",
        "        (\"Text Chunking\", 30, 50, 40, 8),\n",
        "        (\"Vector Embedding\", 30, 40, 40, 8),\n",
        "        (\"Database Storage\", 30, 30, 40, 8),\n",
        "        (\"User Question\", 10, 15, 30, 8),\n",
        "        (\"Retrieval & Context\", 50, 15, 30, 8),\n",
        "        (\"LLM Answer Generation\", 30, 5, 40, 8)\n",
        "    ]\n",
        "\n",
        "    # Add boxes for steps\n",
        "    for i, (name, x, y, width, height) in enumerate(steps):\n",
        "        color = plt.cm.viridis(i / len(steps))\n",
        "        rect = Rectangle((x, y), width, height, facecolor=color, alpha=0.7, edgecolor='black')\n",
        "        ax.add_patch(rect)\n",
        "        ax.text(x + width/2, y + height/2, name, ha='center', va='center', fontweight='bold')\n",
        "\n",
        "    # Add arrows between steps\n",
        "    for i in range(len(steps) - 4):\n",
        "        start_x = steps[i][1] + steps[i][3] / 2\n",
        "        start_y = steps[i][2]\n",
        "        end_x = steps[i+1][1] + steps[i+1][3] / 2\n",
        "        end_y = steps[i+1][2] + steps[i+1][4]\n",
        "        arrow = FancyArrowPatch((start_x, start_y), (end_x, end_y),\n",
        "                            arrowstyle='-|>', mutation_scale=15,\n",
        "                            color='black', linewidth=1.5)\n",
        "        ax.add_patch(arrow)\n",
        "\n",
        "    # Add special arrows for the last connections\n",
        "    arrows = [\n",
        "        (steps[6][1] + steps[6][3]/2, steps[6][2], steps[6][1] + steps[6][3]/2, steps[9][2] + steps[9][4]),\n",
        "        (steps[7][1] + steps[7][3], steps[7][2] + steps[7][4]/2, steps[9][1], steps[9][2] + steps[9][4]/2),\n",
        "        (steps[8][1] + steps[8][3]/2, steps[8][2], steps[9][1] + steps[9][3]/2, steps[9][2] + steps[9][4])\n",
        "    ]\n",
        "\n",
        "    for start_x, start_y, end_x, end_y in arrows:\n",
        "        arrow = FancyArrowPatch((start_x, start_y), (end_x, end_y),\n",
        "                            arrowstyle='-|>', mutation_scale=15,\n",
        "                            color='black', linewidth=1.5)\n",
        "        ax.add_patch(arrow)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('workflow_diagram.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return \"Workflow diagram created\"\n",
        "\n",
        "# Visualization 3: Performance Dashboard\n",
        "def create_performance_dashboard():\n",
        "    \"\"\"Create a dashboard showing system performance metrics\"\"\"\n",
        "    fig = plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Set up a 2x2 grid for the dashboard\n",
        "    gs = fig.add_gridspec(2, 2, hspace=0.3, wspace=0.3)\n",
        "\n",
        "    # Add title\n",
        "    fig.suptitle(\"Medical Assistant Performance Dashboard\", fontsize=20, y=0.98)\n",
        "\n",
        "    # Chart 1: Response Time by Question Type\n",
        "    ax1 = fig.add_subplot(gs[0, 0])\n",
        "    question_types = ['Blood Pressure', 'CPR', 'Diabetes', 'Complex Medical']\n",
        "    response_times = [2.3, 1.8, 2.5, 3.2]\n",
        "\n",
        "    bars = ax1.bar(question_types, response_times, color=plt.cm.viridis(np.linspace(0, 0.8, len(question_types))))\n",
        "    ax1.set_title('Average Response Time by Question Type')\n",
        "    ax1.set_ylabel('Time (seconds)')\n",
        "    ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Add value labels on top of each bar\n",
        "    for i, v in enumerate(response_times):\n",
        "        ax1.text(i, v + 0.1, f\"{v:.1f}s\", ha='center')\n",
        "\n",
        "    # Chart 2: Accuracy by Source\n",
        "    ax2 = fig.add_subplot(gs[0, 1])\n",
        "    sources = ['Blood Pressure Video', 'CPR Video', 'Diabetes Video', 'Combined Knowledge']\n",
        "    accuracy = [0.92, 0.88, 0.95, 0.90]\n",
        "\n",
        "    for i, (source, acc) in enumerate(zip(sources, accuracy)):\n",
        "        ax2.annotate(f\"{acc*100:.0f}%\",\n",
        "                    xy=(i, acc),\n",
        "                    xytext=(0, 10),\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center',\n",
        "                    va='bottom',\n",
        "                    fontweight='bold')\n",
        "\n",
        "    ax2.bar(sources, accuracy, color=plt.cm.plasma(np.linspace(0.2, 0.8, len(sources))))\n",
        "    ax2.set_title('Accuracy by Knowledge Source')\n",
        "    ax2.set_ylabel('Accuracy Score')\n",
        "    ax2.set_ylim(0, 1.0)\n",
        "    ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Chart 3: Response Quality Distribution\n",
        "    ax3 = fig.add_subplot(gs[1, 0])\n",
        "    response_types = ['Comprehensive', 'Accurate but Brief', 'Uncertain', 'Failed']\n",
        "    distribution = [65, 25, 8, 2]\n",
        "\n",
        "    colors = ['#4CAF50', '#2196F3', '#FFC107', '#F44336']\n",
        "    ax3.pie(distribution, labels=response_types, autopct='%1.1f%%', startangle=90, colors=colors,\n",
        "           wedgeprops={'edgecolor': 'white', 'linewidth': 1})\n",
        "    ax3.set_title('Response Quality Distribution')\n",
        "\n",
        "    # Chart 4: System Processing Time Breakdown\n",
        "    ax4 = fig.add_subplot(gs[1, 1])\n",
        "    process_stages = ['Video Download', 'Transcription', 'Vector Indexing',\n",
        "                     'Query Processing', 'Answer Generation']\n",
        "    processing_times = [42, 35, 10, 5, 8]\n",
        "\n",
        "    # Create the horizontal bar chart\n",
        "    bars = ax4.barh(process_stages, processing_times, color=plt.cm.cool(np.linspace(0.2, 0.8, len(process_stages))))\n",
        "    ax4.set_title('System Processing Time Breakdown')\n",
        "    ax4.set_xlabel('Processing Time (%)')\n",
        "    ax4.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Add percentage annotations\n",
        "    for i, (bar, value) in enumerate(zip(bars, processing_times)):\n",
        "        ax4.text(value + 1, bar.get_y() + bar.get_height()/2, f\"{value}%\",\n",
        "                va='center', ha='left', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.savefig('performance_dashboard.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return \"Performance dashboard created\"\n",
        "\n",
        "# Visualization 4: Interactive Demo UI Mockup\n",
        "def create_ui_mockup():\n",
        "    \"\"\"Create a mockup of the user interface\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12, 8))\n",
        "    ax.set_xlim(0, 100)\n",
        "    ax.set_ylim(0, 100)\n",
        "\n",
        "    # Turn off axis\n",
        "    ax.set_axis_off()\n",
        "\n",
        "    # Add background\n",
        "    background = Rectangle((0, 0), 100, 100, facecolor='#F5F5F5')\n",
        "    ax.add_patch(background)\n",
        "\n",
        "    # App header\n",
        "    header = Rectangle((0, 90), 100, 10, facecolor='#2196F3')\n",
        "    ax.add_patch(header)\n",
        "    ax.text(50, 95, \"Smart Medical Assistant\", ha='center', va='center',\n",
        "            color='white', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Chat window\n",
        "    chat_window = Rectangle((10, 20), 80, 65, facecolor='white', edgecolor='#BDBDBD')\n",
        "    ax.add_patch(chat_window)\n",
        "\n",
        "    # Input area\n",
        "    input_area = Rectangle((10, 5), 65, 10, facecolor='white', edgecolor='#BDBDBD')\n",
        "    ax.add_patch(input_area)\n",
        "    ax.text(15, 10, \"Type your medical question here...\", ha='left', va='center',\n",
        "            color='#9E9E9E', fontsize=10)\n",
        "\n",
        "    # Send button\n",
        "    send_button = Rectangle((80, 5), 10, 10, facecolor='#4CAF50', edgecolor='#388E3C')\n",
        "    ax.add_patch(send_button)\n",
        "    ax.text(85, 10, \"Send\", ha='center', va='center', color='white', fontweight='bold')\n",
        "\n",
        "    # Add chat messages\n",
        "    chat_messages = [\n",
        "        # (text, is_user, y_position)\n",
        "        (\"What are the symptoms of diabetes?\", True, 75),\n",
        "        (\"Based on the medical videos, common symptoms of diabetes include:\\n- Increased thirst and urination\\n- Unexplained weight loss\\n- Fatigue\\n- Blurred vision\\n- Slow healing sores\\n- Frequent infections\\n\\nThese symptoms occur because excess sugar in your bloodstream pulls fluid from tissues. Type 1 diabetes symptoms often develop quickly, while Type 2 diabetes symptoms may develop slowly over years.\", False, 55),\n",
        "        (\"How is Type 1 different from Type 2?\", True, 40),\n",
        "        (\"Type 1 diabetes is an autoimmune condition where the pancreas produces little to no insulin. It typically develops in children and young adults.\\n\\nType 2 diabetes is a metabolic disorder where cells become resistant to insulin's effects. It's often associated with lifestyle factors and typically develops in adults.\\n\\nThe key difference is that Type 1 is caused by the body attacking insulin-producing cells, while Type 2 involves the body becoming resistant to insulin.\", False, 25)\n",
        "    ]\n",
        "\n",
        "    for text, is_user, y_pos in chat_messages:\n",
        "        # Determine text position and alignment\n",
        "        if is_user:\n",
        "            bubble_color = \"#E1F5FE\"\n",
        "            text_x = 80\n",
        "            align = \"right\"\n",
        "            bubble_x = 40\n",
        "            bubble_width = 45\n",
        "        else:\n",
        "            bubble_color = \"#FFFFFF\"\n",
        "            text_x = 15\n",
        "            align = \"left\"\n",
        "            bubble_x = 10\n",
        "            bubble_width = 75\n",
        "\n",
        "        # Add chat bubble\n",
        "        lines = text.split('\\n')\n",
        "        bubble_height = 3 + 2 * len(lines)\n",
        "\n",
        "        bubble = Rectangle((bubble_x, y_pos - bubble_height), bubble_width, bubble_height,\n",
        "                          facecolor=bubble_color, edgecolor='#BDBDBD', alpha=0.7,\n",
        "                          linewidth=1, joinstyle='round', capstyle='round')\n",
        "        ax.add_patch(bubble)\n",
        "\n",
        "        # Add text\n",
        "        if is_user:\n",
        "            ax.text(text_x, y_pos - 2, text, ha=align, va='top', wrap=True, fontsize=9)\n",
        "        else:\n",
        "            wrapped = textwrap.fill(text, width=80)\n",
        "            ax.text(text_x, y_pos - 2, wrapped, ha=align, va='top', wrap=True, fontsize=9)\n",
        "\n",
        "    # Add voice input button\n",
        "    voice_circle = Circle((92, 75), 5, facecolor='#FF5722', edgecolor='#E64A19')\n",
        "    ax.add_patch(voice_circle)\n",
        "    ax.text(92, 75, \"🎤\", ha='center', va='center', fontsize=14, color='white')\n",
        "\n",
        "    # Add help button\n",
        "    help_circle = Circle((92, 60), 3, facecolor='#9E9E9E')\n",
        "    ax.add_patch(help_circle)\n",
        "    ax.text(92, 60, \"?\", ha='center', va='center', fontsize=10, fontweight='bold', color='white')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('ui_mockup.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return \"UI mockup created\"\n",
        "\n",
        "# Visualization 5: Knowledge Graph of Medical Concepts\n",
        "def create_knowledge_graph():\n",
        "    \"\"\"Create a visualization of the medical knowledge graph\"\"\"\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=(12, 10), facecolor='white')\n",
        "\n",
        "    # Use NetworkX for graph creation\n",
        "    import networkx as nx\n",
        "\n",
        "    # Create a graph\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Define main nodes\n",
        "    main_nodes = [\"Blood Pressure\", \"CPR\", \"Diabetes\"]\n",
        "\n",
        "    # Add main topics\n",
        "    for node in main_nodes:\n",
        "        G.add_node(node, size=2000, group=main_nodes.index(node))\n",
        "\n",
        "    # Add subtopic nodes\n",
        "    subtopics = {\n",
        "        \"Blood Pressure\": [\"Hypertension\", \"Measurement\", \"Lifestyle\", \"Medication\", \"Symptoms\"],\n",
        "        \"CPR\": [\"Chest Compressions\", \"Rescue Breathing\", \"AED Use\", \"Recovery Position\", \"Emergency Response\"],\n",
        "        \"Diabetes\": [\"Type 1\", \"Type 2\", \"Blood Sugar\", \"Insulin\", \"Diet\", \"Complications\"]\n",
        "    }\n",
        "\n",
        "    # Add all nodes and connections to main topics\n",
        "    for main_topic, topics in subtopics.items():\n",
        "        for topic in topics:\n",
        "            G.add_node(topic, size=1000, group=main_nodes.index(main_topic))\n",
        "            G.add_edge(main_topic, topic, weight=5)\n",
        "\n",
        "    # Add some cross-connections\n",
        "    connections = [\n",
        "        (\"Hypertension\", \"Complications\"),\n",
        "        (\"Blood Sugar\", \"Lifestyle\"),\n",
        "        (\"Medication\", \"Type 2\"),\n",
        "        (\"Diet\", \"Lifestyle\"),\n",
        "        (\"Symptoms\", \"Type 1\"),\n",
        "        (\"Symptoms\", \"Type 2\"),\n",
        "        (\"Emergency Response\", \"AED Use\"),\n",
        "        (\"Hypertension\", \"Type 2\"),\n",
        "        (\"Blood Sugar\", \"Symptoms\")\n",
        "    ]\n",
        "\n",
        "    for source, target in connections:\n",
        "        G.add_edge(source, target, weight=2)\n",
        "\n",
        "    # Layout\n",
        "    pos = nx.spring_layout(G, k=0.3, seed=42)\n",
        "\n",
        "    # Get node groups for coloring\n",
        "    node_groups = [G.nodes[node].get('group', 0) for node in G.nodes()]\n",
        "\n",
        "    # Draw the network\n",
        "    node_sizes = [G.nodes[node].get('size', 500) for node in G.nodes()]\n",
        "\n",
        "    # Set edge properties based on weight\n",
        "    edge_widths = [G[u][v]['weight'] for u, v in G.edges()]\n",
        "\n",
        "    # Draw the graph\n",
        "    nodes = nx.draw_networkx_nodes(G, pos,\n",
        "                                  node_size=node_sizes,\n",
        "                                  node_color=node_groups,\n",
        "                                  cmap=plt.cm.Set1,\n",
        "                                  alpha=0.8,\n",
        "                                  edgecolors='black',\n",
        "                                  linewidths=1)\n",
        "\n",
        "    edges = nx.draw_networkx_edges(G, pos,\n",
        "                                  width=edge_widths,\n",
        "                                  alpha=0.5,\n",
        "                                  edge_color='gray',\n",
        "                                  connectionstyle='arc3,rad=0.1')\n",
        "\n",
        "    # Add labels with adjusted positions\n",
        "    label_pos = {k: (v[0], v[1] + 0.08) for k, v in pos.items()}\n",
        "    nx.draw_networkx_labels(G, label_pos, font_size=10, font_weight='bold')\n",
        "\n",
        "    # Add a title\n",
        "    plt.title(\"Medical Knowledge Concept Graph\", fontsize=20, pad=20)\n",
        "\n",
        "    # Remove axis\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Add a legend\n",
        "    legend_elements = [\n",
        "        plt.Line2D([0], [0], marker='o', color='w', label=topic,\n",
        "                 markerfacecolor=plt.cm.Set1(i), markersize=10)\n",
        "        for i, topic in enumerate(main_nodes)\n",
        "    ]\n",
        "    plt.legend(handles=legend_elements, loc='lower right', title=\"Main Topics\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('knowledge_graph.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    return \"Knowledge graph created\"\n",
        "\n",
        "# Run all the visualizations\n",
        "print(create_system_architecture_diagram())\n",
        "print(create_workflow_diagram())\n",
        "print(create_performance_dashboard())\n",
        "print(create_ui_mockup())\n",
        "print(create_knowledge_graph())"
      ],
      "metadata": {
        "id": "zhVWqrhJqAsq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}